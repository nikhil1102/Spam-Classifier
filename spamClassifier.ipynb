{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = pd.read_csv('C:/Users/nikhi/Desktop/cps/project/spam/spam.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "message1 = [line.rstrip() for line in open('C:/Users/nikhi/Desktop/cps/project/spam/spam.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = message.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
    "message = message.rename(columns = {'v1':'label','v2':'message'})\n",
    "message.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#length of messages\n",
    "message['length'] = message['message'].apply(len)\n",
    "message['label'] = message['label'].map({'ham': 0, 'spam': 1})\n",
    "message.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message  length\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...     161\n",
       "5568      0              Will Ì_ b going to esplanade fr home?      37\n",
       "5569      0  Pity, * was in mood for that. So...any other s...      57\n",
       "5570      0  The guy did some bitching but I acted like i'd...     125\n",
       "5571      0                         Rofl. Its true to its name      26"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17684bcd748>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize the distribution of length\n",
    "message['length'].plot(bins=50,kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using bag-of-words approach, where each unique word in a text will be represented by one number.\n",
    "Steps to convert raw messages(sequence of characters) into vectors (sequence of numbers):\n",
    "          1. Split a message into it individual words and return a list\n",
    "          2. Remove very common words('the','a',etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, until, jurong, point, crazy, available, o...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
       "4    [nah, i, do, n't, think, he, go, to, usf, he, ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def split_into_lemmas(message):\n",
    "    message = message.lower()\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "message['message'].head().apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5562    [Ok, lor, Sony, ericsson, salesman, ask, shuhu...\n",
       "5563                             [Ard, 6, like, dat, lor]\n",
       "5564        [dont, wait, til, least, wednesday, see, get]\n",
       "5565                                           [Huh, lei]\n",
       "5566    [REMINDER, O2, get, 250, pounds, free, call, c...\n",
       "5567    [2nd, time, tried, 2, contact, u, U, å£750, Po...\n",
       "5568                   [Ì, b, going, esplanade, fr, home]\n",
       "5569                     [Pity, mood, Soany, suggestions]\n",
       "5570    [guy, bitching, acted, like, id, interested, b...\n",
       "5571                                   [Rofl, true, name]\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message['message'].tail(10).apply(text_process)\n",
    "# message.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Normalization\n",
    "\n",
    "Lot of ways to continue normalizing this text.\n",
    "Such as Stemming or distinguishing by part of speech.\n",
    "NLTK has lots of build-in tools and great documentation. it doest work well for text-messages due to abbreviations used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "messages as list of tokens (also known as lemmas) and now we need to convert each msg into vectors that ml model can understand.\n",
    "\n",
    "three steps using bag-of-words model:\n",
    "1. Count how many times does a word occur in each message (term frequency)\n",
    "2. weight the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (l2 norm) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector will have many dimensions as they are unique words in the SMS corpus. \n",
    "\n",
    "SciKit Learn's CountVectorizer will convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "2 - Dimensional Matrix\n",
    "One - Dimension would be entire vocabulary (1 row per word) and other dimension are the actual documents, in this case a column per text message.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11304\n"
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer= text_process).fit(message['message'])\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform\n",
    "message_bow = bow_transformer.transform(message['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the Sparse Matrix: (5572, 11304)\n",
      "Non-zero occurences: 50193\n"
     ]
    }
   ],
   "source": [
    "print('shape of the Sparse Matrix:', message_bow.shape)\n",
    "print('Non-zero occurences:', message_bow.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term and normalization can be done with TF-IDF using scikit-learn's TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Term frequency - inverse document frequency. often used in information retrieval and text mining.\n",
    "1. Weight is a statistical measure used to evaluate how imp a word is to a document in a collection or corpus\n",
    "2. Imp increase proportionally to the no of times a word appears in the document but is offset by the freq of the word in the corpus\n",
    "3. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "## TF : Term Frequency:\n",
    "\n",
    "Measures how freq a term occurs in a document. Every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. \n",
    "\n",
    "Term freq is often divided by the document length (i.e. total no of terms in the document) as a wy of normalization:\n",
    "\n",
    "TF(t) = (No of times term t appears in a document)/ (To no of terms in the document)\n",
    "\n",
    "## IDF : Inverse Document Frequency:\n",
    "\n",
    "Measure how important a term is.  While calculating TF, all terms are considered equally imp. \n",
    "\n",
    "However it is known that certain terms, such as 'is', 'of', 'the' may appear a lot of times but have little imp. \n",
    "\n",
    "Thus we need to weigth down the freq terms while scale up the rare ones, by computing the following:\n",
    "\n",
    "IDF(t) = log_e(Total no of documents / No of documents with term t in it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 11304)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(message_bow)\n",
    "message_tfidf = tfidf_transformer.transform(message_bow)\n",
    "print(message_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "\n",
    "We will be using Sklearn's pipeline capabiliter to store a pipeline of workflow.\n",
    "\n",
    "This allows us to set up all the transformations that we will do to the data for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "   ( 'bow',CountVectorizer(analyzer=text_process)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With messages represented as vectors, we can finally train our spam/ham classifier.\n",
    "We will be using Naive Bayes classifier algorithm to build spam classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457 1115\n"
     ]
    }
   ],
   "source": [
    "#split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(message['message'], message['label'],test_size=0.1, random_state=42)\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x00000176838A5AF8>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the train dataset in pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    965\n",
       "1    150\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      1003\n",
      "           1       0.75      1.00      0.85       112\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.87      0.98      0.92      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n",
      "[[965  38]\n",
      " [  0 112]]\n",
      "0.8549618320610688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, f1_score\n",
    "print(classification_report(predictions,y_test))\n",
    "print(confusion_matrix(predictions,y_test))\n",
    "print(f1_score(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8549618320610688\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spam.pkl']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, 'spam.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,render_template,url_for,request\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class MyClassifier(BaseEstimator):\n",
    "\n",
    "    def __init__(self, classifier_type: str = 'rf'):\n",
    "        \"\"\"\n",
    "        A Custome BaseEstimator that can switch between classifiers.\n",
    "        :param classifier_type: string - The switch for different classifiers\n",
    "        \"\"\"\n",
    "        self.classifier_type = classifier_type\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.classifier_type == 'rf':\n",
    "            self.classifier_ = RandomForestClassifier(n_estimators=10)\n",
    "        elif self.classifier_type == 'MultinomialNB':\n",
    "            self.classifier_ = MultinomialNB()\n",
    "        else:\n",
    "            raise ValueError('Unkown classifier type.')\n",
    "\n",
    "        self.classifier_.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.classifier_.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=split_into_lemmas)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf', MyClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   19.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:\n",
      " {'clf__classifier_type': 'MultinomialNB'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "parameter_space = {\n",
    "    'clf__classifier_type': ['rf', 'MultinomialNB']\n",
    "}\n",
    "scorers = {\n",
    "        'precision_score': make_scorer(precision_score),\n",
    "        'recall_score': make_scorer(recall_score),\n",
    "        'accuracy_score': make_scorer(accuracy_score)\n",
    "        }\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "search = GridSearchCV(pipeline1 , parameter_space,verbose=1,cv=3,scoring=scorers,refit=\"precision_score\")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print('Best model:\\n', search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98      1011\n",
      "           1       0.69      1.00      0.82       104\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.85      0.98      0.90      1115\n",
      "weighted avg       0.97      0.96      0.96      1115\n",
      "\n",
      "[[965  46]\n",
      " [  0 104]]\n",
      "0.8188976377952756\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions1,y_test))\n",
    "print(confusion_matrix(predictions1,y_test))\n",
    "print(f1_score(predictions1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# svc = svm.SVC()\n",
    "pipeline_svm = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=split_into_lemmas)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', svm.SVC()),  # <== change here\n",
    "])\n",
    "\n",
    "# pipeline parameters to automatically explore and tune\n",
    "param_svm = [\n",
    "  {'classifier__C': [1, 10, 100, 1000], 'classifier__kernel': ['linear']},\n",
    "  {'classifier__C': [1, 10, 100, 1000], 'classifier__gamma': [0.001, 0.0001], 'classifier__kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    pipeline_svm,  # pipeline from above\n",
    "    param_grid=param_svm,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all data, on the best detected classifier\n",
    "     # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(n_splits=5),  # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "%time svm_detector = grid_svm.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = svm_detector.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       978\n",
      "           1       0.91      1.00      0.95       137\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.96      0.99      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n",
      "[[965  13]\n",
      " [  0 137]]\n",
      "0.9547038327526133\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions2,y_test))\n",
    "print(confusion_matrix(predictions2,y_test))\n",
    "print(f1_score(predictions2, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.99      0.99       975\n",
    "           1       0.91      0.97      0.94       140\n",
    "\n",
    "    accuracy                           0.98      1115\n",
    "   macro avg       0.95      0.98      0.96      1115\n",
    "weighted avg       0.98      0.98      0.98      1115\n",
    "\n",
    "[[961  14]\n",
    " [  4 136]]\n",
    "0.9379310344827586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x00000176838A5AF8>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w...\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma=0.001,\n",
       "                     kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spam.pkl']"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(grid_svm.best_estimator_, 'spam.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from sklearn) (0.24.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.19.4)\n",
      "The scikit-learn version is 0.21.3.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(svm_detector.predict([\"Hi mom, how are you?\"])[0])\n",
    "print(svm_detector.predict([\"WINNER! Credit for free!\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyClassifier' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-294-5beda44f0f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Hi mom, how are you?\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"WINNER! Credit for free!\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelegate_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelegate_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                     \u001b[0mdelegate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelegate_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelegate_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                     \u001b[0mdelegate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MyClassifier' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "print(search.predict_proba([\"Hi mom, how are you?\"])[0])\n",
    "print(search.predict_proba([\"WINNER! Credit for free!\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
